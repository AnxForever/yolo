import os
import cv2
import time
import threading
import queue
from datetime import datetime
from flask import Flask, render_template, Response, jsonify, request, make_response
from flask_cors import CORS

from advanced_face_recognition import AdvancedFaceRecognitionSystem

app = Flask(__name__)
CORS(app)

class UltraOptimizedSystem:
    def __init__(self):
        print("âš¡ è¶…çº§ä¼˜åŒ–ç³»ç»Ÿåˆå§‹åŒ–...")
        self.face_system = AdvancedFaceRecognitionSystem()
        
        # æ‘„åƒå¤´è®¾ç½®
        self.cap = None
        self.is_running = False
        self.camera_index = 0 # è·Ÿè¸ªå½“å‰æ‘„åƒå¤´ç´¢å¼•
        
        # åŒç¼“å†²æœºåˆ¶
        self.display_frame = None
        self.processing_frame = None
        self.frame_lock = threading.RLock()
        
        # è¯†åˆ«ç»“æœ
        self.latest_results = {
            'face_count': 0,
            'face_names': [],
            'face_emotions': [],
            'face_locations': []
        }
        
        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        self.process_every_n_frames = 15  # æ¯15å¸§å¤„ç†ä¸€æ¬¡ï¼ˆå¤§å¹…æå‡æµç•…åº¦ï¼‰
        self.frame_count = 0
        self.last_process_time = 0
        self.min_process_interval = 0.5  # æœ€å°å¤„ç†é—´éš”0.5ç§’
        
        # æƒ…ç»ªæ£€æµ‹å¼€å…³
        self.emotion_detection_enabled = False  # é»˜è®¤å…³é—­ä»¥è·å¾—æœ€ä½³æ€§èƒ½
        self.emotion_detection_frequency = 30   # æƒ…ç»ªæ£€æµ‹é¢‘ç‡ï¼ˆæ¯Nå¸§æ£€æµ‹ä¸€æ¬¡ï¼‰
        self.last_known_emotions = []  # å‚¨å­˜æœ€è¿‘ä¸€æ¬¡æˆåŠŸçš„æƒ…ç»ªåˆ†æç»“æœ
        
        # è‡ªåŠ¨æŠ“æ‹
        self.auto_capture_targets = set()
        self.capture_cooldown = {}
        self.capture_interval = 5
        self.capture_history = []
        
        # ç›®å½•
        self.auto_capture_dir = os.path.join(self.face_system.script_dir, "auto_captures")
        os.makedirs(self.auto_capture_dir, exist_ok=True)
        
        # FPSç»Ÿè®¡
        self.fps = 0
        self.fps_counter = 0
        self.fps_start_time = time.time()
        
        # å¤„ç†çº¿ç¨‹
        self.processing_thread = None
        self.process_queue = queue.Queue(maxsize=2)
        
        print("âœ… è¶…çº§ä¼˜åŒ–ç³»ç»Ÿå°±ç»ª")
    
    def start_camera(self, camera_index=0):
        """å¯åŠ¨é«˜æ€§èƒ½æ‘„åƒå¤´"""
        try:
            if self.is_running:
                return True
            
            print(f"ğŸ“¹ å°è¯•å¯åŠ¨æ‘„åƒå¤´ #{camera_index}...")
            
            # å°è¯•ä¸åŒçš„åç«¯
            for backend in [cv2.CAP_DSHOW, cv2.CAP_MSMF, cv2.CAP_ANY]:
                self.cap = cv2.VideoCapture(camera_index, backend)
                if self.cap and self.cap.isOpened():
                    break
                if self.cap:
                    self.cap.release()
            
            if not self.cap or not self.cap.isOpened():
                print(f"âŒ æ‘„åƒå¤´ #{camera_index} å¯åŠ¨å¤±è´¥")
                return False
            
            self.camera_index = camera_index # æ›´æ–°å½“å‰æ‘„åƒå¤´ç´¢å¼•
            
            # é«˜æ€§èƒ½æ‘„åƒå¤´é…ç½®
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)   # æ›´é«˜åˆ†è¾¨ç‡
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)        # æœ€å°ç¼“å†²
            self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M','J','P','G'))  # MJPEGç¼–ç 
            
            # è·å–å®é™…è®¾ç½®
            actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            actual_fps = self.cap.get(cv2.CAP_PROP_FPS)
            
            print(f"âœ… æ‘„åƒå¤´é…ç½®: {actual_width}x{actual_height} @ {actual_fps}fps")
            
            self.is_running = True
            
            # å¯åŠ¨å¼‚æ­¥å¤„ç†çº¿ç¨‹
            self.processing_thread = threading.Thread(target=self._async_processing_loop, daemon=True)
            self.processing_thread.start()
            
            return True
            
        except Exception as e:
            print(f"âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def stop_camera(self):
        """åœæ­¢æ‘„åƒå¤´"""
        self.is_running = False
        
        if self.processing_thread:
            # self.processing_thread.join(timeout=1.0)
            pass # Let it terminate on its own
        
        if self.cap:
            self.cap.release()
            self.cap = None
        
        print("ğŸ›‘ æ‘„åƒå¤´å·²åœæ­¢")
    
    def switch_camera(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ‘„åƒå¤´"""
        print("ğŸ”„ æ­£åœ¨åˆ‡æ¢æ‘„åƒå¤´...")
        self.stop_camera()
        time.sleep(0.5) # ç»™ç‚¹æ—¶é—´é‡Šæ”¾èµ„æº
        
        # å°è¯•å¯åŠ¨ä¸‹ä¸€ä¸ªæ‘„åƒå¤´ï¼Œå¦‚æœå¤±è´¥å†è¯•ä¸‹ä¸€ä¸ªï¼Œæœ€å¤šè¯•4ä¸ª
        for i in range(1, 5):
            next_index = (self.camera_index + i) % 5 
            if self.start_camera(next_index):
                print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ°æ‘„åƒå¤´ #{self.camera_index}")
                return True
        
        print("âŒ æœªæ‰¾åˆ°å…¶ä»–å¯ç”¨æ‘„åƒå¤´ï¼Œå°†å°è¯•é‡å¯é»˜è®¤æ‘„åƒå¤´")
        self.start_camera(0) # å¦‚æœéƒ½å¤±è´¥ï¼Œå›åˆ°é»˜è®¤
        return False
    
    def _async_processing_loop(self):
        """å¼‚æ­¥å¤„ç†å¾ªç¯ - ç‹¬ç«‹çº¿ç¨‹"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§è¿›è¡Œå¤„ç†
                frame_data = self.process_queue.get(timeout=1.0)
                if frame_data is None:
                    continue
                
                frame, frame_id = frame_data # æ‹†åŒ…ï¼Œè·å–å¸§å’Œå®ƒè‡ªå·±çš„ID
                current_time = time.time()
                
                # æ—¶é—´é—´éš”æ§åˆ¶
                if current_time - self.last_process_time < self.min_process_interval:
                    continue
                
                # è¿›è¡Œäººè„¸æ£€æµ‹å’Œè¯†åˆ«ï¼Œä¼ å…¥æ­£ç¡®çš„å¸§ID
                results = self._process_frame_heavy(frame, frame_id)
                
                # æ›´æ–°ç»“æœ
                with self.frame_lock:
                    self.latest_results = results
                    self.last_process_time = current_time
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ å¼‚æ­¥å¤„ç†é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def _process_frame_heavy(self, frame, frame_id):
        """é‡åº¦å¤„ç† - åœ¨å¼‚æ­¥çº¿ç¨‹ä¸­æ‰§è¡Œ"""
        try:
            # äººè„¸æ£€æµ‹
            face_locations = self.face_system.detect_faces_yolo(frame)
            face_names = []
            face_emotions = []
            
            if face_locations:
                # è½¬æ¢é¢œè‰²ç©ºé—´
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # äººè„¸è¯†åˆ«
                face_names = self.face_system.recognize_faces(rgb_frame, face_locations)
                
                # å¦‚æœæ£€æµ‹åˆ°çš„äººè„¸æ•°é‡ä¸è®°å¿†ä¸­çš„æ•°é‡ä¸ç¬¦ï¼Œåˆ™æ¸…ç©ºè®°å¿†ï¼Œé˜²æ­¢æ•°æ®é”™ä¹±
                if len(face_locations) != len(self.last_known_emotions):
                    self.last_known_emotions = []

                # æ™ºèƒ½æƒ…ç»ªåˆ†æï¼šä½¿ç”¨ä¼ å…¥çš„frame_idè¿›è¡Œåˆ¤æ–­
                if self.emotion_detection_enabled and frame_id % self.emotion_detection_frequency == 0 and face_locations:
                    try:
                        # åˆ†æå¹¶æ›´æ–°"è®°å¿†èŠ¯ç‰‡"
                        analyzed_emotions = self.face_system._analyze_emotions(frame, face_locations)
                        self.last_known_emotions = analyzed_emotions
                        print(f"ğŸ˜Š æƒ…ç»ªè®°å¿†å·²æ›´æ–°: {self.last_known_emotions}")
                    except Exception as e:
                        print(f"âš ï¸ æƒ…ç»ªæ£€æµ‹å¤±è´¥ï¼Œæ¸…ç©ºè®°å¿†: {e}")
                        self.last_known_emotions = [] # åˆ†æå¤±è´¥æ—¶ä¹Ÿæ¸…ç©º

                # æœ€ç»ˆç»“æœï¼šå§‹ç»ˆä½¿ç”¨"è®°å¿†èŠ¯ç‰‡"é‡Œçš„æ•°æ®
                face_emotions = self.last_known_emotions
            
            # æ£€æŸ¥è‡ªåŠ¨æŠ“æ‹
            if face_names:
                self._check_auto_capture(frame, face_names, face_emotions)
            
            return {
                'face_count': len(face_locations),
                'face_names': face_names,
                'face_emotions': face_emotions,
                'face_locations': face_locations
            }
            
        except Exception as e:
            print(f"âŒ é‡åº¦å¤„ç†é”™è¯¯: {e}")
            return {
                'face_count': 0,
                'face_names': [],
                'face_emotions': [],
                'face_locations': []
            }
    
    def generate_frames(self):
        """è¶…é«˜æ€§èƒ½è§†é¢‘æµç”Ÿæˆ"""
        if not self.start_camera():
            return
        
        print("ğŸ¬ å¯åŠ¨è¶…é«˜æ€§èƒ½è§†é¢‘æµ...")
        
        while self.is_running and self.cap:
            try:
                ret, frame = self.cap.read()
                if not ret or frame is None:
                    print("âš ï¸ è¯»å–å¸§å¤±è´¥")
                    time.sleep(0.01)
                    continue
                
                # æ›´æ–°æ˜¾ç¤ºå¸§
                with self.frame_lock:
                    self.display_frame = frame.copy()
                
                self.frame_count += 1
                
                # æ™ºèƒ½å¤„ç†è°ƒåº¦
                current_time = time.time()
                should_process = (
                    self.frame_count % self.process_every_n_frames == 0 and
                    current_time - self.last_process_time >= self.min_process_interval
                )
                
                if should_process:
                    # å¼‚æ­¥æäº¤å¤„ç†ä»»åŠ¡
                    try:
                        frame_copy = frame.copy()
                        self.process_queue.put_nowait((frame_copy, self.frame_count))
                    except queue.Full:
                        pass  # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒï¼Œä¿æŒæµç•…åº¦
                
                # ç»˜åˆ¶ç»“æœåˆ°æ˜¾ç¤ºå¸§
                display_frame = self._draw_lightweight_results(frame)
                
                # é«˜è´¨é‡ç¼–ç 
                encode_params = [
                    cv2.IMWRITE_JPEG_QUALITY, 90,  # æé«˜è´¨é‡
                    cv2.IMWRITE_JPEG_OPTIMIZE, 1,  # ä¼˜åŒ–ç¼–ç 
                ]
                
                ret, buffer = cv2.imencode('.jpg', display_frame, encode_params)
                
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
                # ç®€å•çš„å¸§ç‡æ§åˆ¶
                time.sleep(0.001)  # å¾®å°å»¶è¿Ÿï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                
            except Exception as e:
                print(f"âŒ è§†é¢‘æµé”™è¯¯: {e}")
                time.sleep(0.05)
        
        print("ğŸ è§†é¢‘æµå·²åœæ­¢")
    
    def _draw_lightweight_results(self, frame):
        """è½»é‡çº§ç»“æœç»˜åˆ¶"""
        with self.frame_lock:
            results = self.latest_results.copy()
        
        face_locations = results.get('face_locations', [])
        face_names = results.get('face_names', [])
        face_emotions = results.get('face_emotions', [])
        
        # ç»˜åˆ¶äººè„¸æ¡†å’Œæ ‡ç­¾
        for i, (top, right, bottom, left) in enumerate(face_locations):
            name = face_names[i] if i < len(face_names) else "Unknown"
            emotion = face_emotions[i] if i < len(face_emotions) else ""
            
            # è®¾ç½®é¢œè‰²
            if name != "æœªçŸ¥":
                color = (0, 255, 0)
                if name in self.auto_capture_targets:
                    color = (255, 165, 0)  # æ©™è‰² - è‡ªåŠ¨æŠ“æ‹ç›®æ ‡
            else:
                color = (0, 0, 255)
            
            # ç»˜åˆ¶äººè„¸æ¡† - åŠ ç²—
            cv2.rectangle(frame, (left, top), (right, bottom), color, 3)
            
            # å‡†å¤‡æ–‡æœ¬
            display_text = name
            if emotion:
                display_text = f"{name} ({emotion})"
            if name in self.auto_capture_targets:
                display_text += " [AUTO]"
            
            # è®¡ç®—æ–‡æœ¬å¤§å°
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.7
            thickness = 2
            (text_width, text_height), _ = cv2.getTextSize(display_text, font, font_scale, thickness)
            
            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
            cv2.rectangle(frame, (left, bottom - text_height - 10), 
                         (left + text_width + 10, bottom), color, cv2.FILLED)
            
            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(frame, display_text, (left + 5, bottom - 5), 
                       font, font_scale, (255, 255, 255), thickness)
        
        return frame
    
    def _check_auto_capture(self, frame, face_names, face_emotions):
        """è‡ªåŠ¨æŠ“æ‹æ£€æŸ¥"""
        if not self.auto_capture_targets:
            return
        
        current_time = time.time()
        
        for i, name in enumerate(face_names):
            if name in self.auto_capture_targets:
                last_capture = self.capture_cooldown.get(name, 0)
                if current_time - last_capture >= self.capture_interval:
                    emotion = face_emotions[i] if i < len(face_emotions) else ""
                    self._capture_person(frame, name, emotion)
                    self.capture_cooldown[name] = current_time
    
    def _capture_person(self, frame, person_name, emotion):
        """è‡ªåŠ¨æŠ“æ‹"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ultra_capture_{person_name}_{emotion}_{timestamp}.jpg"
            filepath = os.path.join(self.auto_capture_dir, filename)
            
            # é«˜è´¨é‡ä¿å­˜
            cv2.imwrite(filepath, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            record = {
                'person_name': person_name,
                'emotion': emotion,
                'timestamp': datetime.now().isoformat(),
                'filename': filename
            }
            
            self.capture_history.append(record)
            if len(self.capture_history) > 100:
                self.capture_history = self.capture_history[-100:]
            
            print(f"ğŸ“¸ è¶…çº§æŠ“æ‹: {person_name} ({emotion})")
            
        except Exception as e:
            print(f"âŒ æŠ“æ‹å¤±è´¥: {e}")

# å…¨å±€ç³»ç»Ÿå®ä¾‹
system = UltraOptimizedSystem()

# Flaskè·¯ç”±
@app.route('/')
def index():
    response = make_response(render_template('index.html'))
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    return response

@app.route('/video_feed')
def video_feed():
    print("ğŸ¥ è¯·æ±‚è¶…çº§è§†é¢‘æµ...")
    return Response(system.generate_frames(),
                   mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/api/start_recognition', methods=['POST'])
def start_recognition():
    if system.start_camera(system.camera_index):
        return jsonify({'status': 'success', 'message': 'è¶…çº§è¯†åˆ«å·²å¯åŠ¨'})
    return jsonify({'status': 'error', 'message': 'å¯åŠ¨å¤±è´¥'})

@app.route('/api/stop_recognition', methods=['POST'])
def stop_recognition():
    system.stop_camera()
    return jsonify({'status': 'success', 'message': 'è¯†åˆ«å·²åœæ­¢'})

@app.route('/api/switch_camera', methods=['POST'])
def switch_camera():
    """åˆ‡æ¢æ‘„åƒå¤´"""
    if system.switch_camera():
        return jsonify({'status': 'success', 'message': f'å·²æˆåŠŸåˆ‡æ¢åˆ°æ‘„åƒå¤´ {system.camera_index}'})
    return jsonify({'status': 'error', 'message': 'åˆ‡æ¢æ‘„åƒå¤´å¤±è´¥'})

@app.route('/api/recognition_status')
def recognition_status():
    return jsonify({
        'is_running': system.is_running,
        'results': system.latest_results,
        'auto_capture_targets': list(system.auto_capture_targets),
        'fps': system.fps,
        'emotion_detection_enabled': system.emotion_detection_enabled,
        'emotion_detection_frequency': system.emotion_detection_frequency
    })

@app.route('/api/database_info')
def database_info():
    known_names = list(set(system.face_system.known_face_names))
    return jsonify({
        'known_people': known_names,
        'total_people': len(known_names),
        'total_features': len(system.face_system.known_face_encodings)
    })

@app.route('/api/update_database', methods=['POST'])
def update_database():
    data = request.get_json(silent=True) or {}
    augment = data.get('augment', False)
    
    try:
        system.face_system.rescan_and_encode_database(augment=augment)
        system.face_system.load_face_database()
        return jsonify({'status': 'success', 'message': 'æ•°æ®åº“æ›´æ–°æˆåŠŸ'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'æ›´æ–°å¤±è´¥: {str(e)}'})

@app.route('/api/auto_capture_targets', methods=['GET', 'POST'])
def auto_capture_targets():
    if request.method == 'GET':
        return jsonify({'targets': list(system.auto_capture_targets)})
    
    data = request.get_json(silent=True) or {}
    action = data.get('action')
    person_name = data.get('person_name')
    
    if action == 'add' and person_name:
        system.auto_capture_targets.add(person_name)
        return jsonify({'status': 'success', 'message': f'å·²æ·»åŠ  {person_name}'})
    elif action == 'remove' and person_name:
        system.auto_capture_targets.discard(person_name)
        return jsonify({'status': 'success', 'message': f'å·²ç§»é™¤ {person_name}'})
    elif action == 'clear':
        system.auto_capture_targets.clear()
        return jsonify({'status': 'success', 'message': 'å·²æ¸…ç©ºåˆ—è¡¨'})
    
    return jsonify({'status': 'error', 'message': 'æ— æ•ˆæ“ä½œ'})

@app.route('/api/capture_history')
def capture_history():
    return jsonify({
        'history': system.capture_history[-50:],
        'total_captures': len(system.capture_history)
    })

@app.route('/api/manual_capture', methods=['POST'])
def manual_capture():
    if system.display_frame is not None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ultra_manual_{timestamp}.jpg"
        filepath = os.path.join(system.auto_capture_dir, filename)
        
        cv2.imwrite(filepath, system.display_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        return jsonify({'status': 'success', 'message': f'è¶…çº§æŠ“æ‹: {filename}'})
    
    return jsonify({'status': 'error', 'message': 'æ— æ³•è·å–ç”»é¢'})

# æƒ…ç»ªåˆ†ææ§åˆ¶ç«¯ç‚¹
@app.route('/api/toggle_emotion', methods=['POST'])
def toggle_emotion():
    """åˆ‡æ¢æƒ…ç»ªåˆ†æå¼€å…³"""
    try:
        data = request.get_json(silent=True) or {}
        action = data.get('action', 'toggle')
        
        if action == 'toggle':
            system.emotion_detection_enabled = not system.emotion_detection_enabled
        elif action == 'enable':
            system.emotion_detection_enabled = True
        elif action == 'disable':
            system.emotion_detection_enabled = False
        else:
            return jsonify({'status': 'error', 'message': 'æ— æ•ˆçš„æ“ä½œ'})
        
        status_text = "å·²å¯ç”¨" if system.emotion_detection_enabled else "å·²ç¦ç”¨"
        performance_note = "" if system.emotion_detection_enabled else " (æ€§èƒ½æ¨¡å¼)"
        
        return jsonify({
            'status': 'success', 
            'message': f'æƒ…ç»ªæ£€æµ‹{status_text}{performance_note}',
            'emotion_detection_enabled': system.emotion_detection_enabled
        })
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'è®¾ç½®å¤±è´¥: {str(e)}'})

@app.route('/api/emotion_settings', methods=['GET', 'POST'])
def emotion_settings():
    """æƒ…ç»ªæ£€æµ‹è®¾ç½®"""
    if request.method == 'GET':
        return jsonify({
            'emotion_detection_enabled': system.emotion_detection_enabled,
            'emotion_detection_frequency': system.emotion_detection_frequency,
            'performance_impact': 'é«˜' if system.emotion_detection_frequency < 20 else 'ä¸­' if system.emotion_detection_frequency < 40 else 'ä½'
        })
    
    try:
        data = request.get_json(silent=True) or {}
        
        if 'frequency' in data:
            frequency = int(data['frequency'])
            if 10 <= frequency <= 60:
                system.emotion_detection_frequency = frequency
                impact = 'é«˜' if frequency < 20 else 'ä¸­' if frequency < 40 else 'ä½'
                return jsonify({
                    'status': 'success', 
                    'message': f'æ£€æµ‹é¢‘ç‡å·²è®¾ç½®ä¸ºæ¯{frequency}å¸§ (æ€§èƒ½å½±å“: {impact})',
                    'emotion_detection_frequency': system.emotion_detection_frequency
                })
            else:
                return jsonify({'status': 'error', 'message': 'é¢‘ç‡å¿…é¡»åœ¨10-60ä¹‹é—´'})
        
        return jsonify({'status': 'error', 'message': 'ç¼ºå°‘æœ‰æ•ˆå‚æ•°'})
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'è®¾ç½®å¤±è´¥: {str(e)}'})

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨è¶…çº§ä¼˜åŒ–ç‰ˆæœåŠ¡å™¨...")
    print("âš¡ ä¸“æ³¨äºæµç•…åº¦å’Œç”»è´¨")
    print("ğŸ“± è®¿é—®: http://localhost:5000")
    print("="*50)
    
    try:
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
    except KeyboardInterrupt:
        print("\næ­£åœ¨åœæ­¢...")
        system.stop_camera()
        print("ğŸ‘‹ å·²åœæ­¢") 