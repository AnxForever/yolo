import cv2
import face_recognition
import numpy as np
import os
from ultralytics import YOLO
import json
import pickle
from datetime import datetime
import time
from scipy.ndimage import rotate
from PIL import Image, ImageDraw, ImageFont
from deepface import DeepFace

class AdvancedFaceRecognitionSystem:
    """é«˜çº§äººè„¸è¯†åˆ«ç³»ç»Ÿ - æ•´åˆYOLOæ£€æµ‹å’Œface-recognitionè¯†åˆ«"""
    
    def __init__(self):
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # YOLOæ¨¡å‹è·¯å¾„ï¼ˆä¿å®‰ - è´Ÿè´£æ£€æµ‹äººè„¸ä½ç½®ï¼‰
        self.yolo_model_path = os.path.join(
            self.script_dir,
            "../../",  # è¿”å›ä¸¤çº§åˆ°YOLOæ ¹ç›®å½•
            "ultralytics",
            "runs",
            "detect",
            "wider_full_training_20250630_234750",
            "weights",
            "best.pt"
        )
        
        # äººè„¸æ•°æ®åº“è·¯å¾„
        self.face_database_dir = os.path.join(self.script_dir, "face_database")
        self.encodings_file = os.path.join(self.face_database_dir, "face_encodings.pkl")
        
        # åˆ›å»ºäººè„¸æ•°æ®åº“ç›®å½•
        os.makedirs(self.face_database_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.yolo_model = None
        self.known_face_encodings = []
        self.known_face_names = []
        
        # æ‘„åƒå¤´ä¸ç•Œé¢çŠ¶æ€
        self.available_cameras = []
        self.current_camera_index = -1
        self.cap = None
        
        # FPS è®¡ç®—
        self.prev_frame_time = 0
        self.new_frame_time = 0
        
        # ç•Œé¢æ˜¾ç¤ºè®¾ç½®
        self.font = cv2.FONT_HERSHEY_SIMPLEX
        self.info_panel_height = 90
        self.show_info_panel = True
        
        # æˆªå›¾ä¿å­˜ç›®å½•
        self.screenshots_dir = os.path.join(self.script_dir, "screenshots")
        os.makedirs(self.screenshots_dir, exist_ok=True)

        # åŠ è½½ä¸­æ–‡å­—ä½“
        self.font_path = os.path.join(self.script_dir, "fonts", "simhei.ttf")
        self.name_font = self._load_font(self.font_path, 30)

        # è˜è¯·æƒ…ç»ªè¯†åˆ«ä¸“å®¶ (ä½¿ç”¨DeepFace)
        print("ğŸ™‚ æ­£åœ¨è˜è¯·æƒ…ç»ªè¯†åˆ«ä¸“å®¶ (DeepFace - é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹)...")
        # DeepFace ä¼šåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¿™é‡Œä¸éœ€è¦åˆå§‹åŒ–
        print("âœ… æƒ…ç»ªè¯†åˆ«ä¸“å®¶å·²å‡†å¤‡å°±ç»ªã€‚")

        self._initialize_system()
        
    def _load_font(self, path, size):
        """å®‰å…¨åœ°åŠ è½½å­—ä½“æ–‡ä»¶"""
        try:
            return ImageFont.truetype(path, size)
        except IOError:
            print(f"âŒ å­—ä½“æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
            print("   è¯·ç¡®ä¿ 'simhei.ttf' (æˆ–ç±»ä¼¼ä¸­æ–‡å­—ä½“) å·²æ”¾ç½®åœ¨ 'fonts' æ–‡ä»¶å¤¹ä¸­ã€‚")
            print("   ç¨‹åºå°†æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡ã€‚")
            return None

    def _initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ŒåŠ è½½æ¨¡å‹å¹¶æ£€æµ‹æ‘„åƒå¤´"""
        print("ğŸš€ ç³»ç»Ÿåˆå§‹åŒ–å¼€å§‹...")
        
        # 1. åŠ è½½YOLOæ¨¡å‹
        if not self.load_yolo_model():
            return False # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåˆ™ç»ˆæ­¢

        # 2. åŠ è½½äººè„¸æ•°æ®åº“
        self.load_face_database()
        
        # 3. æ£€æµ‹å¯ç”¨æ‘„åƒå¤´
        print("ğŸ” æ­£åœ¨æ£€æµ‹å¯ç”¨æ‘„åƒå¤´...")
        self.available_cameras = []
        for i in range(5):
            cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)
            if cap.isOpened():
                ret, frame = cap.read()
                if ret and frame is not None:
                    h, w, _ = frame.shape
                    self.available_cameras.append({'index': i, 'width': w, 'height': h})
                    print(f"  âœ… å‘ç°æ‘„åƒå¤´ {i} (åˆ†è¾¨ç‡: {w}x{h})")
                cap.release()
        
        if not self.available_cameras:
            print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°ä»»ä½•å¯ç”¨çš„æ‘„åƒå¤´ã€‚")
            return False
        
        self.current_camera_index = self.available_cameras[0]['index']
        print(f"ğŸ¯ æˆåŠŸæ£€æµ‹åˆ° {len(self.available_cameras)} ä¸ªæ‘„åƒå¤´ï¼Œé»˜è®¤ä½¿ç”¨ç´¢å¼• {self.current_camera_index}")
        print("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        return True

    def load_yolo_model(self):
        """åŠ è½½YOLOäººè„¸æ£€æµ‹æ¨¡å‹ï¼ˆä¿å®‰ï¼‰"""
        try:
            if os.path.exists(self.yolo_model_path):
                self.yolo_model = YOLO(self.yolo_model_path)
                print("  âœ… YOLOäººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                print(f"  âŒ é”™è¯¯: YOLOæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨äº '{self.yolo_model_path}'")
                print("  ğŸ¤” è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
                return False
        except Exception as e:
            print(f"  âŒ é”™è¯¯: YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
        return True

    def load_face_database(self):
        """åŠ è½½äººè„¸è¯†åˆ«æ•°æ®åº“ï¼ˆæ¥å¾…å‘˜çš„åå†Œï¼‰"""
        try:
            if os.path.exists(self.encodings_file):
                with open(self.encodings_file, 'rb') as f:
                    data = pickle.load(f)
                    self.known_face_encodings = data['encodings']
                    self.known_face_names = data['names']
                print(f"  âœ… äººè„¸æ•°æ®åº“åŠ è½½æˆåŠŸï¼Œå…± {len(set(self.known_face_names))} äººï¼Œæ€»è®¡ {len(self.known_face_encodings)} ä¸ªé¢éƒ¨ç‰¹å¾æ ·æœ¬")
            else:
                print("  ğŸ“ ä¿¡æ¯: äººè„¸æ•°æ®åº“ä¸ºç©ºï¼Œè¯·è¿è¡Œæ›´æ–°ç¨‹åºè¿›è¡Œæ³¨å†Œã€‚")
        except Exception as e:
            print(f"  âŒ é”™è¯¯: äººè„¸æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")

    def save_face_database(self):
        """ä¿å­˜äººè„¸è¯†åˆ«æ•°æ®åº“"""
        try:
            data = {
                'encodings': self.known_face_encodings,
                'names': self.known_face_names
            }
            with open(self.encodings_file, 'wb') as f:
                pickle.dump(data, f)
            print("  âœ… äººè„¸æ•°æ®åº“ä¿å­˜æˆåŠŸ")
            return True
        except Exception as e:
            print(f"  âŒ é”™è¯¯: äººè„¸æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            return False

    def _get_rotated_images(self, image):
        """
        å¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œæ•°æ®å¢å¼ºï¼Œç”Ÿæˆæ—‹è½¬åçš„å‰¯æœ¬ã€‚
        """
        augmented_images = []
        # åŸå§‹å›¾åƒå¿…é¡»ä¿ç•™
        augmented_images.append(image)
        
        # å®šä¹‰æ—‹è½¬è§’åº¦
        angles = [-15, 15, 90, -90, 180]
        
        for angle in angles:
            # ä½¿ç”¨scipyè¿›è¡Œå›¾åƒæ—‹è½¬ï¼Œreshape=Falseä¿æŒåŸå°ºå¯¸
            rotated_image = rotate(image, angle, reshape=False)
            augmented_images.append(rotated_image)
            
        return augmented_images

    def rescan_and_encode_database(self, augment: bool = False):
        """
        å…¨é¢æ‰«æface_databaseæ–‡ä»¶å¤¹ï¼Œä¸ºæ¯ä¸ªå­æ–‡ä»¶å¤¹ï¼ˆä»£è¡¨ä¸€ä¸ªäººï¼‰ä¸­çš„æ‰€æœ‰ç…§ç‰‡åˆ›å»ºç¼–ç ã€‚
        è¿™å°†è¦†ç›–ç°æœ‰çš„æ•°æ®åº“ã€‚
        :param augment: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬å›¾åƒï¼‰ä»¥æå‡è§’åº¦è¯†åˆ«èƒ½åŠ›ã€‚
        """
        if augment:
            print("\n--- âœ¨ å¼€å§‹ [å¢å¼ºæ¨¡å¼] æ‰«æå¹¶æ›´æ–°äººè„¸æ•°æ®åº“ (è¿‡ç¨‹è¾ƒæ…¢) ---")
        else:
            print("\n--- ğŸ”„ å¼€å§‹ [æ ‡å‡†æ¨¡å¼] æ‰«æå¹¶æ›´æ–°äººè„¸æ•°æ®åº“ ---")
            
        self.known_face_encodings.clear()
        self.known_face_names.clear()
        
        subfolders = [f.path for f in os.scandir(self.face_database_dir) if f.is_dir()]
        
        if not subfolders:
            print("âš ï¸ è­¦å‘Šï¼šåœ¨ face_database ä¸­æœªæ‰¾åˆ°ä»»ä½•äººå‘˜æ–‡ä»¶å¤¹ï¼ˆå­ç›®å½•ï¼‰ã€‚")

        total_features_count = 0
        for person_dir in subfolders:
            person_name = os.path.basename(person_dir)
            print(f"ğŸ“ æ­£åœ¨å¤„ç†äººå‘˜: {person_name}")
            
            image_count = 0
            for image_name in os.listdir(person_dir):
                image_path = os.path.join(person_dir, image_name)
                
                if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    try:
                        original_image = face_recognition.load_image_file(image_path)
                        
                        images_to_process = []
                        if augment:
                            images_to_process = self._get_rotated_images(original_image)
                            print(f"  - å¢å¼ºç…§ç‰‡: {image_name} (ç”Ÿæˆ {len(images_to_process)} ä¸ªç‰ˆæœ¬)")
                        else:
                            images_to_process = [original_image]

                        # å¤„ç†åŸå§‹åŠæ‰€æœ‰å¢å¼ºåçš„å›¾åƒ
                        for i, image in enumerate(images_to_process):
                            # æ­¥éª¤1: å¼ºåˆ¶ä½¿ç”¨YOLOæ¥æ£€æµ‹äººè„¸ä½ç½®
                            face_locations = self.detect_faces_yolo(image)

                            if face_locations:
                                # æ­¥éª¤2: å¦‚æœYOLOæ‰¾åˆ°äº†äººè„¸ï¼Œå°±è®©face_recognitionåœ¨æŒ‡å®šä½ç½®æå–ç‰¹å¾
                                encodings = face_recognition.face_encodings(image, known_face_locations=face_locations)
                                
                                if encodings:
                                    for encoding in encodings:
                                        self.known_face_encodings.append(encoding)
                                        self.known_face_names.append(person_name)
                                        total_features_count += 1
                                    if i == 0: # åªåœ¨å¤„ç†åŸå§‹å›¾ç‰‡æ—¶æ‰“å°ä¸»å­¦ä¹ ä¿¡æ¯
                                        print(f"    - âœ… [YOLOå®šä½æˆåŠŸ] å·²å­¦ä¹ : {image_name} (å‘ç° {len(encodings)} ä¸ªé¢éƒ¨ç‰¹å¾)")
                            elif i == 0: # å¦‚æœåŸå§‹å›¾ç‰‡YOLOéƒ½æ£€æµ‹ä¸åˆ°ï¼Œå°±å‘Šè­¦
                                print(f"    - âš ï¸  [YOLOå®šä½å¤±è´¥] åœ¨åŸå§‹ç…§ç‰‡ {image_name} ä¸­æœªæ£€æµ‹åˆ°äººè„¸ï¼Œå·²è·³è¿‡ã€‚")
                        
                        image_count += 1
                            
                    except Exception as e:
                        print(f"  - âŒ é”™è¯¯: å¤„ç† {image_name} æ—¶å¤±è´¥: {e}")
            
            if image_count == 0:
                 print(f"  - âš ï¸ è­¦å‘Š: æ–‡ä»¶å¤¹ '{person_name}' ä¸­æ²¡æœ‰ä»»ä½•å¯å­¦ä¹ çš„ç…§ç‰‡ã€‚")
                        
        if self.known_face_encodings:
            self.save_face_database()
            print(f"\n--- âœ… æ•°æ®åº“æ›´æ–°å®Œæˆï¼å…± {len(set(self.known_face_names))} äºº, {total_features_count} ä¸ªé¢éƒ¨ç‰¹å¾æ ·æœ¬å·²ä¿å­˜ã€‚ ---")
        else:
            print("\n--- âš ï¸ æ•°æ®åº“ä¸ºç©ºæˆ–æœªå­¦åˆ°ä»»ä½•æœ‰æ•ˆç‰¹å¾ï¼Œæœªä¿å­˜ä»»ä½•å†…å®¹ã€‚ ---")

    def detect_faces_yolo(self, frame):
        """ä½¿ç”¨YOLOæ£€æµ‹äººè„¸ä½ç½®ï¼ˆä¿å®‰çš„å·¥ä½œï¼‰"""
        if self.yolo_model is None:
            return []
        
        try:
            results = self.yolo_model(frame, verbose=False)
            face_locations = []
            
            if results and results[0].boxes is not None:
                for box in results[0].boxes:
                    # è·å–è¾¹ç•Œæ¡†åæ ‡
                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                    # è½¬æ¢ä¸ºface_recognitionåº“ä½¿ç”¨çš„æ ¼å¼ (top, right, bottom, left)
                    face_locations.append((y1, x2, y2, x1))
            
            return face_locations
        except Exception as e:
            print(f"âŒ YOLOäººè„¸æ£€æµ‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    def recognize_faces(self, frame_rgb, face_locations):
        """è¯†åˆ«äººè„¸èº«ä»½ï¼ˆæ¥å¾…å‘˜çš„å·¥ä½œï¼‰"""
        # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œç›´æ¥è¿”å›"æœªçŸ¥"
        if not self.known_face_encodings:
            return ["æœªçŸ¥"] * len(face_locations)
        
        try:
            # æå–å½“å‰å¸§ä¸­æ‰€æœ‰äººè„¸çš„ç‰¹å¾
            current_face_encodings = face_recognition.face_encodings(frame_rgb, face_locations)
            
            face_names = []
            for face_encoding in current_face_encodings:
                # å°†å½“å‰äººè„¸ä¸æ•°æ®åº“ä¸­çš„æ‰€æœ‰äººè„¸è¿›è¡Œæ¯”è¾ƒ
                matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding, tolerance=0.5)
                name = "æœªçŸ¥"
                
                # è®¡ç®—ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸çš„è·ç¦»ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸€ä¸ª
                face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)
                
                if len(face_distances) > 0:
                    best_match_index = np.argmin(face_distances)
                    # å¦‚æœæœ€ç›¸ä¼¼çš„äººè„¸ç¡®å®åŒ¹é…ï¼Œå¹¶ä¸”è·ç¦»è¶³å¤Ÿè¿‘
                    if matches[best_match_index]:
                        name = self.known_face_names[best_match_index]
                
                face_names.append(name)
            
            return face_names
        except Exception as e:
            print(f"âŒ äººè„¸è¯†åˆ«æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return ["é”™è¯¯"] * len(face_locations)

    def _draw_info_panel(self, frame, fps, face_count):
        """åœ¨ç”»é¢ä¸Šç»˜åˆ¶ä¿¡æ¯é¢æ¿"""
        if not self.show_info_panel:
            return frame

        h, w, _ = frame.shape
        # åˆ›å»ºä¸€ä¸ªåŠé€æ˜çš„é»‘è‰²èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, h - self.info_panel_height), (w, h), (0, 0, 0), -1)
        alpha = 0.6  # é€æ˜åº¦
        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)

        # ç»„ç»‡è¦æ˜¾ç¤ºçš„ä¿¡æ¯
        cam_info = self.available_cameras[self.current_camera_index]
        info_lines = [
            f"FPS: {fps:.1f}",
            f"Faces: {face_count}",
            f"Camera: {cam_info['index']} ({cam_info['width']}x{cam_info['height']})",
            "Controls: [C] Change Cam | [S] Screenshot | [H] Hide Panel | [Q] Quit"
        ]
        
        # é€è¡Œç»˜åˆ¶ä¿¡æ¯
        for i, line in enumerate(info_lines):
            y_pos = h - self.info_panel_height + 25 + (i * 20)
            cv2.putText(frame, line, (10, y_pos), self.font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
            
        return frame

    def _save_snapshot(self, frame):
        """ä¿å­˜å½“å‰å¸§çš„å¿«ç…§"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"snapshot_{timestamp}_faces_{len(self.detect_faces_yolo(frame))}.jpg"
        filepath = os.path.join(self.screenshots_dir, filename)
        cv2.imwrite(filepath, frame)
        print(f"ğŸ“¸ å¿«ç…§å·²ä¿å­˜: {filepath}")

    def _switch_camera(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ‘„åƒå¤´"""
        if len(self.available_cameras) <= 1:
            print("â„¹ï¸ åªæœ‰ä¸€ä¸ªå¯ç”¨æ‘„åƒå¤´ï¼Œæ— æ³•åˆ‡æ¢ã€‚")
            return
            
        # è®¡ç®—ä¸‹ä¸€ä¸ªæ‘„åƒå¤´çš„ç´¢å¼•
        current_cam_list_index = next((i for i, cam in enumerate(self.available_cameras) if cam['index'] == self.current_camera_index), None)
        
        if current_cam_list_index is None:
            # å¦‚æœå½“å‰æ‘„åƒå¤´ç´¢å¼•ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰ï¼Œå®‰å…¨èµ·è§åˆ‡æ¢å›ç¬¬ä¸€ä¸ª
            next_cam_list_index = 0
        else:
            # æ­£å¸¸å¾ªç¯åˆ‡æ¢
            next_cam_list_index = (current_cam_list_index + 1) % len(self.available_cameras)
            
        self.current_camera_index = self.available_cameras[next_cam_list_index]['index']
        
        # é‡Šæ”¾æ—§çš„æ‘„åƒå¤´å¯¹è±¡
        if self.cap:
            self.cap.release()
            
        # åˆ›å»ºæ–°çš„æ‘„åƒå¤´å¯¹è±¡
        self.cap = cv2.VideoCapture(self.current_camera_index, cv2.CAP_DSHOW)
        if not self.cap.isOpened():
            print(f"âŒ é”™è¯¯: æ— æ³•åˆ‡æ¢åˆ°æ‘„åƒå¤´ {self.current_camera_index}")
            self.cap = None
        else:
            print(f"ğŸ”„ å·²åˆ‡æ¢åˆ°æ‘„åƒå¤´ {self.current_camera_index}")

    def _analyze_emotions(self, frame, face_locations):
        """ä½¿ç”¨DeepFaceåº“åˆ†æäººè„¸æƒ…ç»ª"""
        emotions_result = []
        for (top, right, bottom, left) in face_locations:
            # ç²¾ç¡®è£å‰ªå‡ºäººè„¸åŒºåŸŸ
            # æ³¨æ„ï¼šè¦ç¡®ä¿åæ ‡ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
            top = max(0, top)
            left = max(0, left)
            bottom = min(frame.shape[0], bottom)
            right = min(frame.shape[1], right)
            
            cropped_face = frame[top:bottom, left:right]

            if cropped_face.size == 0:
                emotions_result.append("")
                continue

            # ä½¿ç”¨DeepFaceæ£€æµ‹æƒ…ç»ª
            try:
                # DeepFace.analyze è¿”å›æƒ…ç»ªåˆ†æç»“æœ
                result = DeepFace.analyze(cropped_face, actions=['emotion'], enforce_detection=False)
                if result and len(result) > 0:
                    # DeepFaceè¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªç»“æœ
                    emotion_data = result[0]['emotion']
                    # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æƒ…ç»ª
                    top_emotion = max(emotion_data, key=emotion_data.get)
                    emotions_result.append(top_emotion)
                else:
                    emotions_result.append("") # æœªæ£€æµ‹åˆ°æƒ…ç»ª
            except Exception as e:
                # print(f"æƒ…ç»ªåˆ†ææ—¶å‡ºé”™: {e}") # è°ƒè¯•æ—¶å¯æ‰“å¼€
                emotions_result.append("") # å‡ºé”™æ—¶ä¹Ÿè¿”å›ç©º
        
        return emotions_result

    def _draw_results_on_frame(self, frame, face_locations, face_names, face_emotions):
        """ä½¿ç”¨Pillowåœ¨ç”»é¢ä¸Šç»˜åˆ¶ç»“æœï¼Œæ”¯æŒä¸­æ–‡å’Œæƒ…ç»ª"""
        # å°†OpenCVå›¾åƒ(BGR)è½¬æ¢ä¸ºPillowå›¾åƒ(RGB)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        draw = ImageDraw.Draw(pil_image)

        for (top, right, bottom, left), name, emotion in zip(face_locations, face_names, face_emotions):
            # ç»˜åˆ¶äººè„¸æ¡†
            draw.rectangle(((left, top), (right, bottom)), outline=(0, 255, 0), width=2)
            
            # ç»„åˆæœ€ç»ˆæ˜¾ç¤ºçš„æ–‡æœ¬
            display_text = name
            if emotion:
                display_text = f"{name} ({emotion})"

            # åˆ¤æ–­æ˜¯å¦åŠ è½½äº†å­—ä½“
            if self.name_font:
                # ä½¿ç”¨Pillowç»˜åˆ¶å¸¦ä¸­æ–‡çš„æ ‡ç­¾
                text_size = draw.textbbox((0, 0), display_text, font=self.name_font)
                text_width = text_size[2] - text_size[0]
                text_height = text_size[3] - text_size[1]
                
                label_bottom = bottom
                label_top = label_bottom - text_height - 10
                label_right = left + text_width + 10
                
                draw.rectangle(((left, label_top), (label_right, label_bottom)), fill=(0, 255, 0))
                draw.text((left + 5, label_top + 5), display_text, font=self.name_font, fill=(0, 0, 0))
            else:
                # å­—ä½“åŠ è½½å¤±è´¥æ—¶çš„åå¤‡æ–¹æ¡ˆ
                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)
                cv2.putText(frame, display_text, (left + 6, bottom - 6), self.font, 0.7, (0, 0, 0), 2)


        # å°†Pillowå›¾åƒè½¬æ¢å›OpenCVå›¾åƒ(BGR)
        return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

    def run_realtime_recognition(self):
        """è¿è¡Œå®æ—¶äººè„¸æ£€æµ‹ä¸è¯†åˆ«çš„ä¸»å¾ªç¯"""
        if self.current_camera_index == -1:
            print("âŒ æ— æ³•å¯åŠ¨ï¼šç³»ç»ŸæœªæˆåŠŸåˆå§‹åŒ–æˆ–æœªæ‰¾åˆ°æ‘„åƒå¤´ã€‚")
            return
            
        self.cap = cv2.VideoCapture(self.current_camera_index, cv2.CAP_DSHOW)
        if not self.cap.isOpened():
            print(f"âŒ é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.current_camera_index}")
            return
            
        print("\n--- å®æ—¶è¯†åˆ«å·²å¯åŠ¨ ---")
        print("åœ¨è§†é¢‘çª—å£æŒ‰ 'Q' é€€å‡º")

        while True:
            if self.cap is None:
                break
                
            ret, frame = self.cap.read()
            if not ret:
                print("âš ï¸ æ— æ³•ä»æ‘„åƒå¤´è¯»å–ç”»é¢ï¼Œå¯èƒ½å·²æ–­å¼€è¿æ¥ã€‚")
                break
            
            # 1. è®¡ç®—FPS
            self.new_frame_time = time.time()
            fps = 1 / (self.new_frame_time - self.prev_frame_time)
            self.prev_frame_time = self.new_frame_time
            
            # 2. YOLOäººè„¸æ£€æµ‹
            face_locations = self.detect_faces_yolo(frame)
            face_count = len(face_locations)
            
            face_names = []
            face_emotions = []
            if face_count > 0:
                # è½¬æ¢é¢œè‰²ç©ºé—´ä»¥ä¾›face_recognitionä½¿ç”¨
                rgb_small_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # 3. è¯†åˆ«äººè„¸
                face_names = self.recognize_faces(rgb_small_frame, face_locations)

                # 4. åˆ†ææƒ…ç»ª
                face_emotions = self._analyze_emotions(frame, face_locations)

            # 5. åœ¨ç”»é¢ä¸Šç»˜åˆ¶ç»“æœ
            frame = self._draw_results_on_frame(frame, face_locations, face_names, face_emotions)

            # 6. ç»˜åˆ¶ä¿¡æ¯é¢æ¿
            frame = self._draw_info_panel(frame, fps, face_count)

            # 7. æ˜¾ç¤ºæœ€ç»ˆç”»é¢
            cv2.imshow("å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿ", frame)

            # 8. å¤„ç†é”®ç›˜è¾“å…¥
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("ğŸ‘‹ æ­£åœ¨å…³é—­ç³»ç»Ÿ...")
                break
            elif key == ord('s'):
                self._save_snapshot(frame)
            elif key == ord('c'):
                self._switch_camera()
            elif key == ord('h'):
                self.show_info_panel = not self.show_info_panel
        
        # æ¸…ç†èµ„æº
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        print("âœ… ç³»ç»Ÿå·²æˆåŠŸå…³é—­ã€‚")

    def process_single_image(self, image_path):
        """å¤„ç†å•å¼ å›¾åƒ"""
        try:
            print(f"ğŸ–¼ï¸ æ­£åœ¨å¤„ç†å•å¼ å›¾ç‰‡: {image_path}")
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–å›¾ç‰‡ {image_path}")
                return

            # YOLOäººè„¸æ£€æµ‹
            face_locations = self.detect_faces_yolo(frame)
            print(f"  æ£€æµ‹åˆ° {len(face_locations)} ä¸ªäººè„¸")

            face_names = []
            if face_locations:
                # è¯†åˆ«äººè„¸
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                face_names = self.recognize_faces(rgb_frame, face_locations)

            # ç»˜åˆ¶ç»“æœ
            for (top, right, bottom, left), name in zip(face_locations, face_names):
                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)
                cv2.putText(frame, name, (left + 6, bottom - 6), self.font, 1.0, (0, 0, 0), 2)
            
            # ä¿å­˜ç»“æœ
            base, ext = os.path.splitext(os.path.basename(image_path))
            output_path = os.path.join(self.screenshots_dir, f"{base}_recognized{ext}")
            cv2.imwrite(output_path, frame)
            print(f"  âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

            # æ˜¾ç¤ºç»“æœ
            cv2.imshow("å›¾ç‰‡è¯†åˆ«ç»“æœ", frame)
            print("  åœ¨å›¾ç‰‡çª—å£æŒ‰ä»»æ„é”®é€€å‡º...")
            cv2.waitKey(0)
            cv2.destroyAllWindows()

        except Exception as e:
            print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨åº”ç”¨"""
    face_system = AdvancedFaceRecognitionSystem()

    if face_system.yolo_model is None:
        print("\nâŒ ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶åŠ è½½å¤±è´¥ï¼Œç¨‹åºå·²é€€å‡ºã€‚è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return

    while True:
        print("\n" + "="*50)
        print("          é«˜çº§äººè„¸è¯†åˆ«ç³»ç»Ÿ - ä¸»èœå•")
        print("="*50)
        print("1. ğŸš€ å¯åŠ¨å®æ—¶äººè„¸è¯†åˆ«")
        print("2. ğŸ”„ [æ ‡å‡†] æ›´æ–°äººè„¸æ•°æ®åº“ (å¿«é€Ÿ)")
        print("3. âœ¨ [å¢å¼º] æ›´æ–°äººè„¸æ•°æ®åº“ (æå‡è§’åº¦è¯†åˆ«ï¼Œè¾ƒæ…¢)")
        print("4. ğŸ‘‹ é€€å‡ºç³»ç»Ÿ")
        print("-"*50)

        choice = input("è¯·é€‰æ‹©æ“ä½œ (1-4): ").strip()

        if choice == '1':
            if not face_system.known_face_encodings:
                print("\nâš ï¸ è­¦å‘Š: äººè„¸æ•°æ®åº“ä¸ºç©ºæˆ–æœªåŠ è½½ã€‚")
                print("   è¯·å…ˆå°†ç…§ç‰‡æ”¾å…¥ 'face_database' ä¸‹çš„ä¸ªäººæ–‡ä»¶å¤¹ä¸­ï¼Œç„¶åè¿è¡Œæ›´æ–°æ•°æ®åº“é€‰é¡¹ã€‚")
                continue
            
            print("\nå³å°†å¯åŠ¨å®æ—¶è¯†åˆ«...")
            face_system.run_realtime_recognition()
            print("\nè¿”å›ä¸»èœå•...")

        elif choice == '2':
            face_system.rescan_and_encode_database(augment=False)
            print("\næ•°æ®åº“å·²æ›´æ–°ã€‚æ­£åœ¨è‡ªåŠ¨é‡æ–°åŠ è½½æ•°æ®...")
            face_system.load_face_database()

        elif choice == '3':
            face_system.rescan_and_encode_database(augment=True)
            print("\næ•°æ®åº“å·²é€šè¿‡å¢å¼ºæ¨¡å¼æ›´æ–°ã€‚æ­£åœ¨è‡ªåŠ¨é‡æ–°åŠ è½½æ•°æ®...")
            face_system.load_face_database()
            
        elif choice == '4':
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1, 2, 3 æˆ– 4ã€‚")


if __name__ == "__main__":
    main() 