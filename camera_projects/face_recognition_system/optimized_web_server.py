import os
import cv2
import json
import time
import base64
import threading
import queue
import logging
from datetime import datetime
from collections import deque
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
import psutil
import numpy as np

from flask import Flask, render_template, Response, jsonify, request
from flask_cors import CORS

from advanced_face_recognition import AdvancedFaceRecognitionSystem

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    fps: float = 0.0
    processing_time: float = 0.0
    queue_size: int = 0
    memory_usage: float = 0.0
    cpu_usage: float = 0.0
    total_frames: int = 0
    dropped_frames: int = 0
    recognition_accuracy: float = 0.0
    last_update: float = 0.0

@dataclass
class VideoFrame:
    """è§†é¢‘å¸§æ•°æ®ç±»"""
    frame: np.ndarray
    timestamp: float
    frame_id: int
    metadata: Dict[str, Any] = None

class OptimizedCameraManager:
    """ä¼˜åŒ–çš„æ‘„åƒå¤´ç®¡ç†å™¨"""
    
    def __init__(self, camera_index: int = 0, target_fps: int = 30):
        self.camera_index = camera_index
        self.target_fps = target_fps
        self.frame_interval = 1.0 / target_fps
        
        self.cap: Optional[cv2.VideoCapture] = None
        self.is_running = False
        self.capture_thread: Optional[threading.Thread] = None
        
        # å¸§é˜Ÿåˆ— - ä½¿ç”¨æœ‰é™é˜Ÿåˆ—é¿å…å†…å­˜è†¨èƒ€
        self.frame_queue = queue.Queue(maxsize=5)
        self.latest_frame: Optional[VideoFrame] = None
        
        # æ€§èƒ½ç›‘æ§
        self.metrics = PerformanceMetrics()
        self.fps_counter = deque(maxlen=30)  # æœ€è¿‘30å¸§ç”¨äºè®¡ç®—FPS
        self.frame_counter = 0
        
        # çº¿ç¨‹é”
        self.lock = threading.RLock()
        
    def start(self) -> bool:
        """å¯åŠ¨æ‘„åƒå¤´"""
        try:
            if self.is_running:
                logger.warning("æ‘„åƒå¤´å·²åœ¨è¿è¡Œ")
                return True
                
            # åˆå§‹åŒ–æ‘„åƒå¤´
            self.cap = cv2.VideoCapture(self.camera_index, cv2.CAP_DSHOW)
            if not self.cap or not self.cap.isOpened():
                logger.error(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {self.camera_index}")
                return False
            
            # è®¾ç½®æ‘„åƒå¤´å‚æ•°
            self._configure_camera()
            
            # å¯åŠ¨æ•è·çº¿ç¨‹
            self.is_running = True
            self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
            self.capture_thread.start()
            
            logger.info(f"æ‘„åƒå¤´ {self.camera_index} å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ‘„åƒå¤´å¤±è´¥: {e}")
            return False
    
    def stop(self):
        """åœæ­¢æ‘„åƒå¤´"""
        try:
            self.is_running = False
            
            if self.capture_thread:
                self.capture_thread.join(timeout=2.0)
                
            if self.cap:
                self.cap.release()
                self.cap = None
                
            # æ¸…ç©ºé˜Ÿåˆ—
            while not self.frame_queue.empty():
                try:
                    self.frame_queue.get_nowait()
                except queue.Empty:
                    break
                    
            logger.info("æ‘„åƒå¤´å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢æ‘„åƒå¤´æ—¶å‡ºé”™: {e}")
    
    def _configure_camera(self):
        """é…ç½®æ‘„åƒå¤´å‚æ•°"""
        if not self.cap:
            return
            
        # è®¾ç½®åˆ†è¾¨ç‡ - æ ¹æ®æ€§èƒ½è‡ªé€‚åº”
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        # è®¾ç½®FPS
        self.cap.set(cv2.CAP_PROP_FPS, self.target_fps)
        
        # è®¾ç½®ç¼“å†²åŒºå¤§å° - å‡å°‘å»¶è¿Ÿ
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        # è·å–å®é™…è®¾ç½®çš„å‚æ•°
        actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = self.cap.get(cv2.CAP_PROP_FPS)
        
        logger.info(f"æ‘„åƒå¤´é…ç½®: {actual_width}x{actual_height} @ {actual_fps}fps")
    
    def _capture_loop(self):
        """æ‘„åƒå¤´æ•è·å¾ªç¯"""
        last_capture_time = 0
        
        while self.is_running and self.cap:
            try:
                current_time = time.time()
                
                # å¸§ç‡æ§åˆ¶
                if current_time - last_capture_time < self.frame_interval:
                    time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ 
                    continue
                
                # æ•è·å¸§
                ret, frame = self.cap.read()
                if not ret or frame is None:
                    logger.warning("æ— æ³•ä»æ‘„åƒå¤´è¯»å–å¸§")
                    self._handle_capture_error()
                    continue
                
                # åˆ›å»ºè§†é¢‘å¸§å¯¹è±¡
                video_frame = VideoFrame(
                    frame=frame.copy(),
                    timestamp=current_time,
                    frame_id=self.frame_counter,
                    metadata={'capture_time': current_time}
                )
                
                # æ›´æ–°æœ€æ–°å¸§
                with self.lock:
                    self.latest_frame = video_frame
                    self.frame_counter += 1
                
                # å°è¯•å°†å¸§æ”¾å…¥é˜Ÿåˆ—
                try:
                    self.frame_queue.put_nowait(video_frame)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„å¸§
                    try:
                        self.frame_queue.get_nowait()
                        self.frame_queue.put_nowait(video_frame)
                        self.metrics.dropped_frames += 1
                    except queue.Empty:
                        pass
                
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self._update_performance_metrics(current_time)
                last_capture_time = current_time
                
            except Exception as e:
                logger.error(f"æ•è·å¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def _handle_capture_error(self):
        """å¤„ç†æ•è·é”™è¯¯ - å°è¯•é‡æ–°è¿æ¥"""
        logger.warning("æ£€æµ‹åˆ°æ•è·é”™è¯¯ï¼Œå°è¯•é‡æ–°è¿æ¥æ‘„åƒå¤´...")
        
        if self.cap:
            self.cap.release()
            
        time.sleep(1.0)  # ç­‰å¾…ä¸€ç§’åé‡è¯•
        
        self.cap = cv2.VideoCapture(self.camera_index, cv2.CAP_DSHOW)
        if self.cap and self.cap.isOpened():
            self._configure_camera()
            logger.info("æ‘„åƒå¤´é‡æ–°è¿æ¥æˆåŠŸ")
        else:
            logger.error("æ‘„åƒå¤´é‡æ–°è¿æ¥å¤±è´¥")
    
    def _update_performance_metrics(self, current_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.fps_counter.append(current_time)
        
        # è®¡ç®—FPS
        if len(self.fps_counter) > 1:
            time_span = self.fps_counter[-1] - self.fps_counter[0]
            if time_span > 0:
                self.metrics.fps = (len(self.fps_counter) - 1) / time_span
        
        # æ›´æ–°å…¶ä»–æŒ‡æ ‡
        self.metrics.queue_size = self.frame_queue.qsize()
        self.metrics.total_frames = self.frame_counter
        self.metrics.last_update = current_time
        
        # æ¯ç§’æ›´æ–°ä¸€æ¬¡ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        if current_time - self.metrics.last_update > 1.0:
            try:
                process = psutil.Process()
                self.metrics.memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                self.metrics.cpu_usage = process.cpu_percent()
            except:
                pass
    
    def get_latest_frame(self) -> Optional[VideoFrame]:
        """è·å–æœ€æ–°å¸§"""
        with self.lock:
            return self.latest_frame
    
    def get_frame_for_processing(self) -> Optional[VideoFrame]:
        """è·å–ç”¨äºå¤„ç†çš„å¸§"""
        try:
            return self.frame_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_metrics(self) -> PerformanceMetrics:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics
    
    def switch_camera(self, new_index: int) -> bool:
        """åˆ‡æ¢æ‘„åƒå¤´"""
        try:
            was_running = self.is_running
            
            if was_running:
                self.stop()
                time.sleep(0.5)  # ç­‰å¾…å®Œå…¨åœæ­¢
            
            self.camera_index = new_index
            
            if was_running:
                return self.start()
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ‡æ¢æ‘„åƒå¤´å¤±è´¥: {e}")
            return False

class OptimizedWebFaceRecognitionSystem:
    """ä¼˜åŒ–çš„Webäººè„¸è¯†åˆ«ç³»ç»Ÿ"""
    
    def __init__(self):
        self.face_system = AdvancedFaceRecognitionSystem()
        self.camera_manager = OptimizedCameraManager()
        
        # å¤„ç†çº¿ç¨‹
        self.processing_thread: Optional[threading.Thread] = None
        self.is_processing = False
        
        # ç»“æœç¼“å­˜
        self.latest_results: Dict[str, Any] = {}
        self.results_lock = threading.RLock()
        
        # è‡ªåŠ¨æŠ“æ‹åŠŸèƒ½
        self.auto_capture_targets = set()
        self.capture_cooldown = {}
        self.capture_interval = 5
        self.capture_history = []
        
        # åˆ›å»ºç›®å½•
        self.auto_capture_dir = os.path.join(self.face_system.script_dir, "auto_captures")
        os.makedirs(self.auto_capture_dir, exist_ok=True)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.processing_times = deque(maxlen=50)
        
    def start_recognition(self) -> bool:
        """å¯åŠ¨è¯†åˆ«ç³»ç»Ÿ"""
        try:
            # å¯åŠ¨æ‘„åƒå¤´
            if not self.camera_manager.start():
                return False
            
            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            self.is_processing = True
            self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
            self.processing_thread.start()
            
            logger.info("è¯†åˆ«ç³»ç»Ÿå¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨è¯†åˆ«ç³»ç»Ÿå¤±è´¥: {e}")
            return False
    
    def stop_recognition(self):
        """åœæ­¢è¯†åˆ«ç³»ç»Ÿ"""
        try:
            self.is_processing = False
            
            if self.processing_thread:
                self.processing_thread.join(timeout=2.0)
            
            self.camera_manager.stop()
            
            logger.info("è¯†åˆ«ç³»ç»Ÿå·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢è¯†åˆ«ç³»ç»Ÿå¤±è´¥: {e}")
    
    def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.is_processing:
            try:
                # è·å–å¾…å¤„ç†çš„å¸§
                video_frame = self.camera_manager.get_frame_for_processing()
                if video_frame is None:
                    time.sleep(0.01)
                    continue
                
                start_time = time.time()
                
                # è¿›è¡Œäººè„¸æ£€æµ‹å’Œè¯†åˆ«
                results = self._process_frame(video_frame)
                
                # æ£€æŸ¥è‡ªåŠ¨æŠ“æ‹
                if results['face_names']:
                    self._check_auto_capture(video_frame.frame, results)
                
                # æ›´æ–°ç»“æœç¼“å­˜
                with self.results_lock:
                    self.latest_results = results
                
                # æ›´æ–°å¤„ç†æ—¶é—´ç»Ÿè®¡
                processing_time = time.time() - start_time
                self.processing_times.append(processing_time)
                
                # æ›´æ–°æ‘„åƒå¤´æ€§èƒ½æŒ‡æ ‡
                metrics = self.camera_manager.get_metrics()
                metrics.processing_time = processing_time
                
            except Exception as e:
                logger.error(f"å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def _process_frame(self, video_frame: VideoFrame) -> Dict[str, Any]:
        """å¤„ç†å•å¸§"""
        frame = video_frame.frame
        
        # äººè„¸æ£€æµ‹
        face_locations = self.face_system.detect_faces_yolo(frame)
        face_names = []
        face_emotions = []
        
        if face_locations:
            # è½¬æ¢è‰²å½©ç©ºé—´
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # äººè„¸è¯†åˆ«
            face_names = self.face_system.recognize_faces(rgb_frame, face_locations)
            
            # æƒ…ç»ªåˆ†æï¼ˆå¯ä»¥é€‰æ‹©æ€§ç¦ç”¨ä»¥æé«˜æ€§èƒ½ï¼‰
            try:
                face_emotions = self.face_system._analyze_emotions(frame, face_locations)
            except Exception as e:
                logger.warning(f"æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
                face_emotions = [""] * len(face_locations)
        
        return {
            'face_locations': face_locations,
            'face_names': face_names,
            'face_emotions': face_emotions,
            'face_count': len(face_locations),
            'timestamp': video_frame.timestamp,
            'frame_id': video_frame.frame_id
        }
    
    def _check_auto_capture(self, frame: np.ndarray, results: Dict[str, Any]):
        """æ£€æŸ¥è‡ªåŠ¨æŠ“æ‹"""
        if not self.auto_capture_targets:
            return
        
        current_time = time.time()
        
        for i, name in enumerate(results['face_names']):
            if name in self.auto_capture_targets:
                last_capture_time = self.capture_cooldown.get(name, 0)
                if current_time - last_capture_time >= self.capture_interval:
                    self._capture_person(frame, results, i, name)
                    self.capture_cooldown[name] = current_time
    
    def _capture_person(self, frame: np.ndarray, results: Dict[str, Any], person_index: int, person_name: str):
        """è‡ªåŠ¨æŠ“æ‹äººå‘˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            emotion = results['face_emotions'][person_index] if person_index < len(results['face_emotions']) else "unknown"
            
            filename = f"auto_capture_{person_name}_{emotion}_{timestamp}.jpg"
            filepath = os.path.join(self.auto_capture_dir, filename)
            
            # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
            cv2.imwrite(filepath, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            # è®°å½•æŠ“æ‹å†å²
            capture_record = {
                'person_name': person_name,
                'emotion': emotion,
                'timestamp': datetime.now().isoformat(),
                'filename': filename,
                'filepath': filepath
            }
            
            self.capture_history.append(capture_record)
            
            # ä¿æŒå†å²è®°å½•æ•°é‡
            if len(self.capture_history) > 200:
                self.capture_history = self.capture_history[-200:]
            
            logger.info(f"è‡ªåŠ¨æŠ“æ‹: {person_name} ({emotion}) -> {filename}")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æŠ“æ‹å¤±è´¥: {e}")
    
    def generate_video_stream(self):
        """ç”Ÿæˆä¼˜åŒ–çš„è§†é¢‘æµ"""
        while True:
            try:
                # è·å–æœ€æ–°å¸§
                video_frame = self.camera_manager.get_latest_frame()
                if video_frame is None:
                    time.sleep(0.033)  # ~30fps
                    continue
                
                frame = video_frame.frame.copy()
                
                # è·å–æœ€æ–°è¯†åˆ«ç»“æœ
                with self.results_lock:
                    results = self.latest_results.copy()
                
                # åœ¨å¸§ä¸Šç»˜åˆ¶ç»“æœ
                if results:
                    frame = self._draw_results_on_frame(frame, results)
                
                # ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡
                frame = self._draw_performance_overlay(frame)
                
                # ç¼–ç ä¸ºJPEG
                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 80]
                ret, buffer = cv2.imencode('.jpg', frame, encode_param)
                
                if ret:
                    frame_bytes = buffer.tobytes()
                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                
            except Exception as e:
                logger.error(f"è§†é¢‘æµç”Ÿæˆé”™è¯¯: {e}")
                time.sleep(0.1)
    
    def _draw_results_on_frame(self, frame: np.ndarray, results: Dict[str, Any]) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶è¯†åˆ«ç»“æœ"""
        face_locations = results.get('face_locations', [])
        face_names = results.get('face_names', [])
        face_emotions = results.get('face_emotions', [])
        
        for i, (top, right, bottom, left) in enumerate(face_locations):
            # è·å–ä¿¡æ¯
            name = face_names[i] if i < len(face_names) else "Unknown"
            emotion = face_emotions[i] if i < len(face_emotions) else ""
            
            # è®¾ç½®é¢œè‰²
            if name != "æœªçŸ¥":
                color = (0, 255, 0)  # ç»¿è‰² - è¯†åˆ«æˆåŠŸ
                if name in self.auto_capture_targets:
                    color = (255, 165, 0)  # æ©™è‰² - è‡ªåŠ¨æŠ“æ‹ç›®æ ‡
            else:
                color = (0, 0, 255)  # çº¢è‰² - æœªçŸ¥äººå‘˜
            
            # ç»˜åˆ¶äººè„¸æ¡†
            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
            
            # å‡†å¤‡æ˜¾ç¤ºæ–‡æœ¬
            display_text = name
            if emotion:
                display_text = f"{name} ({emotion})"
            if name in self.auto_capture_targets:
                display_text += " [AUTO]"
            
            # ç»˜åˆ¶æ ‡ç­¾
            label_size = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(frame, (left, bottom - 35), (left + label_size[0] + 10, bottom), color, cv2.FILLED)
            cv2.putText(frame, display_text, (left + 5, bottom - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return frame
    
    def _draw_performance_overlay(self, frame: np.ndarray) -> np.ndarray:
        """ç»˜åˆ¶æ€§èƒ½å åŠ ä¿¡æ¯"""
        metrics = self.camera_manager.get_metrics()
        
        # åˆ›å»ºåŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        h, w = frame.shape[:2]
        cv2.rectangle(overlay, (w - 200, 10), (w - 10, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ç»˜åˆ¶æ€§èƒ½ä¿¡æ¯
        info_lines = [
            f"FPS: {metrics.fps:.1f}",
            f"Queue: {metrics.queue_size}",
            f"Frames: {metrics.total_frames}",
            f"Dropped: {metrics.dropped_frames}",
        ]
        
        for i, line in enumerate(info_lines):
            y_pos = 30 + i * 20
            cv2.putText(frame, line, (w - 190, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return frame
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        metrics = self.camera_manager.get_metrics()
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        avg_processing_time = 0
        if self.processing_times:
            avg_processing_time = sum(self.processing_times) / len(self.processing_times)
        
        with self.results_lock:
            results = self.latest_results.copy()
        
        return {
            'is_running': self.is_processing,
            'camera_fps': metrics.fps,
            'processing_time_ms': avg_processing_time * 1000,
            'queue_size': metrics.queue_size,
            'memory_usage_mb': metrics.memory_usage,
            'cpu_usage_percent': metrics.cpu_usage,
            'total_frames': metrics.total_frames,
            'dropped_frames': metrics.dropped_frames,
            'results': results,
            'auto_capture_targets': list(self.auto_capture_targets)
        }

# åˆ›å»ºå…¨å±€ç³»ç»Ÿå®ä¾‹
web_system = OptimizedWebFaceRecognitionSystem()

# Flask è·¯ç”±ï¼ˆä¿æŒåŸæœ‰APIæ¥å£ï¼‰
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/video_feed')
def video_feed():
    return Response(web_system.generate_video_stream(),
                   mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/api/start_recognition', methods=['POST'])
def start_recognition():
    if web_system.start_recognition():
        return jsonify({'status': 'success', 'message': 'äººè„¸è¯†åˆ«å·²å¯åŠ¨'})
    return jsonify({'status': 'error', 'message': 'å¯åŠ¨å¤±è´¥'})

@app.route('/api/stop_recognition', methods=['POST'])
def stop_recognition():
    web_system.stop_recognition()
    return jsonify({'status': 'success', 'message': 'äººè„¸è¯†åˆ«å·²åœæ­¢'})

@app.route('/api/recognition_status')
def recognition_status():
    return jsonify(web_system.get_system_status())

@app.route('/api/database_info')
def database_info():
    known_names = list(set(web_system.face_system.known_face_names))
    return jsonify({
        'known_people': known_names,
        'total_people': len(known_names),
        'total_features': len(web_system.face_system.known_face_encodings)
    })

@app.route('/api/update_database', methods=['POST'])
def update_database():
    data = request.get_json()
    augment = data.get('augment', False)
    
    try:
        web_system.face_system.rescan_and_encode_database(augment=augment)
        web_system.face_system.load_face_database()
        return jsonify({'status': 'success', 'message': 'æ•°æ®åº“æ›´æ–°æˆåŠŸ'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(e)}'})

@app.route('/api/auto_capture_targets', methods=['GET', 'POST'])
def auto_capture_targets():
    if request.method == 'GET':
        return jsonify({'targets': list(web_system.auto_capture_targets)})
    
    elif request.method == 'POST':
        data = request.get_json()
        action = data.get('action')
        person_name = data.get('person_name')
        
        if action == 'add' and person_name:
            web_system.auto_capture_targets.add(person_name)
            return jsonify({'status': 'success', 'message': f'å·²æ·»åŠ  {person_name} åˆ°è‡ªåŠ¨æŠ“æ‹åˆ—è¡¨'})
        
        elif action == 'remove' and person_name:
            web_system.auto_capture_targets.discard(person_name)
            return jsonify({'status': 'success', 'message': f'å·²ä»è‡ªåŠ¨æŠ“æ‹åˆ—è¡¨ç§»é™¤ {person_name}'})
        
        elif action == 'clear':
            web_system.auto_capture_targets.clear()
            return jsonify({'status': 'success', 'message': 'å·²æ¸…ç©ºè‡ªåŠ¨æŠ“æ‹åˆ—è¡¨'})
        
        return jsonify({'status': 'error', 'message': 'æ— æ•ˆçš„æ“ä½œ'})

@app.route('/api/capture_history')
def capture_history():
    return jsonify({
        'history': web_system.capture_history[-50:],
        'total_captures': len(web_system.capture_history)
    })

@app.route('/api/manual_capture', methods=['POST'])
def manual_capture():
    video_frame = web_system.camera_manager.get_latest_frame()
    if video_frame:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"manual_capture_{timestamp}.jpg"
        filepath = os.path.join(web_system.auto_capture_dir, filename)
        
        cv2.imwrite(filepath, video_frame.frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        return jsonify({'status': 'success', 'message': f'æ‰‹åŠ¨æŠ“æ‹æˆåŠŸ: {filename}'})
    
    return jsonify({'status': 'error', 'message': 'æ— æ³•è·å–å½“å‰ç”»é¢'})

@app.route('/api/switch_camera', methods=['POST'])
def switch_camera():
    data = request.get_json()
    camera_index = data.get('camera_index', 0)
    
    if web_system.camera_manager.switch_camera(camera_index):
        return jsonify({'status': 'success', 'message': f'å·²åˆ‡æ¢åˆ°æ‘„åƒå¤´ {camera_index}'})
    return jsonify({'status': 'error', 'message': 'åˆ‡æ¢æ‘„åƒå¤´å¤±è´¥'})

if __name__ == '__main__':
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ä¼˜åŒ–ç‰ˆWebäººè„¸è¯†åˆ«ç³»ç»Ÿ...")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    try:
        app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
    except KeyboardInterrupt:
        print("\næ­£åœ¨ä¼˜é›…å…³é—­ç³»ç»Ÿ...")
        web_system.stop_recognition()
        print("ğŸ‘‹ ç³»ç»Ÿå·²å®‰å…¨åœæ­¢") 